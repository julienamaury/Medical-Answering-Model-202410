MAM（MedQA）

Project environment dependency

The project mainly relies on the following conda environment


conda activate llama_factory

Complete package dependency reference/requirements.txt


Hardware and system level driver dependencies

Hardware and driver dependencies: | Software package/hardware | Version number/configuration | Remarks | | ---------- | --------- | | System | Ubuntu 20.04.1 LTS | | CPU | Intel (R) Xeon (R) Gold 6338 CPU @ 2.00GHz | 2 blocks | | Memory | Samsung, DRAM, 2933 MT/s, 64 GB | 16 64GB memory modules, totaling 1024GB | GPU | A100-SXM4-80GB | 8-card cluster. Each training/inference task uses 1 card | | NVIDIA Driver | 535.104.05 | NVIDIA Linux-x86_64-535.104.05/run, which requires an administrator to install | CUDA | 12.2 | CUDA_12.2.2/535.104.05_inux.run, which requires an administrator to install | CUDNN | 8.9.2.26 | CUDNN-linux-x86_64-8.9.2.26_cuda12-archive. tar.xz, which requires an administrator to install | nccl | 2.18.3 | nccl_2.18.3-1+CUDA12.2_x86_64.txz, which requires an administrator to install|


Software package version dependency

Among them, the core software package version is: | Software Package | Version Number | Remarks | |: ---------- |: --------- |: --------- | | Python | 3.10.14 | Required for this project | | Torch | 2.3.0 | Required for this project | | Transformers | 4.40.2 | Required for this project | | datasets | 2.19.1 | Required for this project | | Accelerate | 0.30.0 | Required for this project | PEFT | 0.10.0 | Required for this project | | trl | 0.8.6 | Unrelated for RLHF use, not used in this project, but the llmtuner library depends on this | | Deepspeed | 0.14.0 | Required for this project | | Bitsandbytes | 0.40 3.1 | Required for this project | | flash attn | 2.5.8 | Required for this project, may require manually running pip install flash attn=2.5.8 separately, installation is very time-consuming, and there may be a possibility of failure, Installation difficulty is high | | vllm | 0.4.2 | This project requires the core deployment library, which may require running pip install vllm==0.4.2 separately. Installation is very time-consuming and there may be a possibility of failure. Installation difficulty is high, please refer to: https://docs.vllm.ai/en/latest/getting_started/installation.html|| llmtuner | 0.7.1.dev0 | This project requires a core training library, which may need to be installed using git clone. Please refer tohttps://github.com/hiyouga/LLaMA-FactoryYou can refer to the tutorial during installationhttps://zhuanlan.zhihu.com/p/695287607|


Environment variable dependencies for calling large model interfaces

This project relies on the use of large models (gpt-4-1106 preview, kimichat, etc.)/data/BigFiveCPED/utils/MultiProcessingLLM。 You canhttps://platform.moonshot.cnThehttps://api2d.comRegister an account application API and use LLM


This section is mainly used when calling the KimiChat API. If you need to call those large models, set those APIs. Before use, environment variables need to be configured as follows: 


Set environment variables

vim ~/.bashrc

Add the following statement and save it


#Set Accesshttps://portal.azure.comThe api_key variable of the ChatGPT interface service provided

# https://zhishenggpt.openai.azure.com/

export GPT35_AZURE_OPENAI_KEY='xxxx'

# https://zhishenggpt40.openai.azure.com/

export GPT4_AZURE_OPENAI_KEY='xxxx'


# https://openai.api2d.net/v1

export API2D_OPENAI_KEY='xxxx'


# https://dev.iai007.cloud/ai/api/v1

export HEFEI_OPENAI_KEY='xxxx'


# https://platform.moonshot.cn/console/api-keys

export KIMI_OPENAI_KEY='xxxx'

Then refresh the environment configuration


source ~/.bashrc

Large model parameter download (recommended)

Suggested usehttps://hf-mirror.com/Download the large model and refer to method three: using hfd, which can achieve stable download without interruption. The example is as follows: 


Set environment variables

vim ~/.bashrc

Add the following statement and save it


export HF_ENDPOINT=https://hf-mirror.com

Then refresh the environment configuration


source ~/.bashrc

Download hfd tool

CD<Your path to save the large model>

wget https://hf-mirror.com/hfd/hfd.shchmoda+x hfd.sh

chmod a+x hfd.sh

Run download command

CD<Your path to save the large model>

./hfd.sh shenzhi-wang/Llama3-8B-Chinese-Chat --tool aria2c -x 4

./hfd.sh Qwen/Qwen1.5-14B-Chat --tool aria2c -x 4

MedQA dataset

Dataset source: https://github.com/jind11/MedQA

Download link: https://drive.google.com/file/d/1ImYUSLk9JbgHXOemfvyiDiirluZHPeQw/view?usp=sharing

Chinese Introduction: https://zhuanlan.zhihu.com/p/679590312

Dataset preprocessing

Convert txt text to json text (completed, no need to repeat operation)

conda activate llama_factory

cd ./data/MedQA/utils


#Convert txt to json

python -u txt2json.py --input_txt_dir '../data_clean/textbooks/zh_paragraph' --output_json_dir '../data_clean/textbooks/zh_paragraph_json' --max_knowledge_len 1800 --language_type chinese --min_knowledge_len 5 --do_chunk

python -u txt2json.py --input_txt_dir '../data_clean/textbooks/zh_sentence' --output_json_dir '../data_clean/textbooks/zh_sentence_json' --max_knowledge_len 1800 --language_type chinese --min_knowledge_len 5 --do_chunk

python -u txt2json.py --input_txt_dir '../data_clean/textbooks/en' --output_json_dir '../data_clean/textbooks/en_json' --max_knowledge_len 1800 --language_type english --min_knowledge_len 5 --do_chunk

Read JSON text to complete vectorization (completed, no need to repeat operation)

#Convert from JSON file to vector

python -u vector_store.py --input_json_dir '../data_clean/textbooks/zh_sentence_json' --store_top_path '../data_clean/vector_stores/zh_sentence' 

python -u vector_store.py --input_json_dir '../data_clean/textbooks/zh_paragraph_json' --store_top_path '../data_clean/vector_stores/zh_paragraph'

python -u vector_store.py --input_json_dir '../data_clean/textbooks/en_json' --store_top_path '../data_clean/vector_stores/en'

Retrieve the training and testing sets and build a standard instruction set (completed, no need for repeated operations)

Specifically, the -- Device_ids parameter specifies the graphics card to be used. If there is only one graphics card, set -- Device_ids="0" and -- num_decess=1. 


Specifically, -- query_key_name specifies the query used for retrieval, which can be selected as "question" - search only with questions, or "questionw_ith_options" - search by concatenating questions and options together. The final plan used 'questionw_ith_options'. 


Specifically, it needs to be opened first/Modify 'model_cath' to the actual path for retrieving the Embedding model from the server in data/MedQA/tiles/config. py. 


conda activate llama_factory

cd ./data/MedQA/utils


#Retrieve JSON files

#Chinese dataset - Retrieval using only "question" 

#Training set

python generate_question_with_knowledges.py \

--num_process=8 \

--index_key_name="id" \

--topk_knowledge=10 \

--knowledge_threshold=0.65 \

--input_jsonl_path="../data_clean/questions/Mainland/train.jsonl" \

--embedding_model_name="stella-base-zh-v2" \

--store_path="../data_clean/vector_stores/zh_paragraph" \

--output_json_dir="../data_clean/questions_with_knowledge/Mainland/train" \

--output_json_file="train.json" \

--device_ids="0,1,2,3,4,5,6,7" \

--query_key_name="question" 


#Verification set

python generate_question_with_knowledges.py \

--num_process=8 \

--index_key_name="id" \

--topk_knowledge=10 \

--knowledge_threshold=0.65 \

--input_jsonl_path="../data_clean/questions/Mainland/dev.jsonl" \

--embedding_model_name="stella-base-zh-v2" \

--store_path="../data_clean/vector_stores/zh_paragraph" \

--output_json_dir="../data_clean/questions_with_knowledge/Mainland/dev" \

--output_json_file="dev.json" \

--device_ids="0,1,2,3,4,5,6,7" \

--query_key_name="question" 


python generate_question_with_knowledges.py \

--num_process=8 \

--index_key_name="id" \

--topk_knowledge=10 \

--knowledge_threshold=0.65 \

--input_jsonl_path="../data_clean/questions/Mainland/test.jsonl" \

--embedding_model_name="stella-base-zh-v2" \

--store_path="../data_clean/vector_stores/zh_paragraph" \

--output_json_dir="../data_clean/questions_with_knowledge/Mainland/test" \

--output_json_file="test.json" \

--device_ids="0,1,2,3,4,5,6,7" \

--query_key_name="question" 



#Chinese dataset - Retrieval using "questionw_ith_options" 

python generate_question_with_knowledges.py \

--num_process=8 \

--index_key_name="id" \

--topk_knowledge=10 \

--knowledge_threshold=0.65 \

--input_jsonl_path="../data_clean/questions/Mainland/train.jsonl" \

--embedding_model_name="stella-base-zh-v2" \

--store_path="../data_clean/vector_stores/zh_paragraph" \

--output_json_dir="../data_clean/questions_with_knowledge/Mainland/train" \

--output_json_file="train.json" \

--device_ids="0,1,2,3,4,5,6,7" \

--query_key_name="question_with_options" 


python generate_question_with_knowledges.py \

--num_process=8 \

--index_key_name="id" \

--topk_knowledge=10 \

--knowledge_threshold=0.65 \

--input_jsonl_path="../data_clean/questions/Mainland/dev.jsonl" \

--embedding_model_name="stella-base-zh-v2" \

--store_path="../data_clean/vector_stores/zh_paragraph" \

--output_json_dir="../data_clean/questions_with_knowledge/Mainland/dev" \

--output_json_file="dev.json" \

--device_ids="0,1,2,3,4,5,6,7" \

--query_key_name="question_with_options" 


python generate_question_with_knowledges.py \

--num_process=8 \

--index_key_name="id" \

--topk_knowledge=10 \

--knowledge_threshold=0.65 \

--input_jsonl_path="../data_clean/questions/Mainland/test.jsonl" \

--embedding_model_name="stella-base-zh-v2" \

--store_path="../data_clean/vector_stores/zh_paragraph" \

--output_json_dir="../data_clean/questions_with_knowledge/Mainland/test" \

--output_json_file="test.json" \

--device_ids="0,1,2,3,4,5,6,7" \

--query_key_name="question_with_options" 



#English Dataset - Retrieval using only "question" 

python generate_question_with_knowledges.py \

--num_process=8 \

--index_key_name="id" \

--topk_knowledge=10 \

--knowledge_threshold=0.65 \

--input_jsonl_path="../data_clean/questions/US/train.jsonl" \

--embedding_model_name="stella-base-en-v2" \

--store_path="../data_clean/vector_stores/en" \

--output_json_dir="../data_clean/questions_with_knowledge/US/train" \

--output_json_file="train.json" \

--device_ids="0,1,2,3,4,5,6,7" \

--query_key_name="question" 


python generate_question_with_knowledges.py \

--num_process=8 \

--index_key_name="id" \

--topk_knowledge=10 \

--knowledge_threshold=0.65 \

--input_jsonl_path="../data_clean/questions/US/dev.jsonl" \

--embedding_model_name="stella-base-en-v2" \

--store_path="../data_clean/vector_stores/en" \

--output_json_dir="../data_clean/questions_with_knowledge/US/dev" \

--output_json_file="dev.json" \

--device_ids="0,1,2,3,4,5,6,7" \

--query_key_name="question" 


python generate_question_with_knowledges.py \

--num_process=8 \

--index_key_name="id" \

--topk_knowledge=10 \

--knowledge_threshold=0.65 \

--input_jsonl_path="../data_clean/questions/US/test.jsonl" \

--embedding_model_name="stella-base-en-v2" \

--store_path="../data_clean/vector_stores/en" \

--output_json_dir="../data_clean/questions_with_knowledge/US/test" \

--output_json_file="test.json" \

--device_ids="0,1,2,3,4,5,6,7" \

--query_key_name="question" 



#English Dataset - Retrieve using only 'questionw_ith_options'

python generate_question_with_knowledges.py \

--num_process=8 \

--index_key_name="id" \

--topk_knowledge=10 \

--knowledge_threshold=0.65 \

--input_jsonl_path="../data_clean/questions/US/train.jsonl" \

--embedding_model_name="stella-base-en-v2" \

--store_path="../data_clean/vector_stores/en" \

--output_json_dir="../data_clean/questions_with_knowledge/US/train" \

--output_json_file="train.json" \

--device_ids="0,1,2,3,4,5,6,7" \

--query_key_name="question_with_options" 




python generate_question_with_knowledges.py \

--num_process=8 \

--index_key_name="id" \

--topk_knowledge=10 \

--knowledge_threshold=0.65 \

--input_jsonl_path="../data_clean/questions/US/dev.jsonl" \

--embedding_model_name="stella-base-en-v2" \

--store_path="../data_clean/vector_stores/en" \

--output_json_dir="../data_clean/questions_with_knowledge/US/dev" \

--output_json_file="dev.json" \

--device_ids="0,1,2,3,4,5,6,7" \

--query_key_name="question_with_options" 



python generate_question_with_knowledges.py \

--num_process=8 \

--index_key_name="id" \

--topk_knowledge=10 \

--knowledge_threshold=0.65 \

--input_jsonl_path="../data_clean/questions/US/test.jsonl" \

--embedding_model_name="stella-base-en-v2" \

--store_path="../data_clean/vector_stores/en" \

--output_json_dir="../data_clean/questions_with_knowledge/US/test" \

--output_json_file="test.json" \

--device_ids="0,1,2,3,4,5,6,7" \

--query_key_name="question_with_options" 

Generate parsed text for Chinese dataset (completed, no need to repeat operation)

conda activate llama_factory

cd ./data/MedQA/utils


#Training set

python ./MultiProcessingLLM/multiprocess_using_chatgpt_input_with_prompt_and_json_data.py --do_check \

--num_process=30 \

--model_name="moonshot-v1-32k_kimi" \

--prompt_config_path="./MultiProcessingLLM/prompt_config_of_generate_explain.json" \

--use_load_raw_json_data_with_process \

--input_json_data_path="../data_clean/questions_with_knowledge/Mainland/train/retrive_use_question_with_options/stella-base-zh-v2/train.json" \

--output_json_dir="../data_clean/questions_with_knowledge/Mainland/train/retrive_use_question_with_options/stella-base-zh-v2/train_with_explain_using_moonshot-v1-32k_kimi" \

--list_placeholder="list_placeholder" \

--llm_output_key="chatgpt_explain" \

--index_key_name="id" \

--temperature=0.7 \

--max_tokens=4096 \

--top_p=0.95


#Test set

python ./MultiProcessingLLM/multiprocess_using_chatgpt_input_with_prompt_and_json_data.py --do_check \

--num_process=30 \

--model_name="moonshot-v1-32k_kimi" \

--prompt_config_path="./MultiProcessingLLM/prompt_config_of_generate_explain.json" \

--use_load_raw_json_data_with_process \

--input_json_data_path="../data_clean/questions_with_knowledge/Mainland/test/retrive_use_question_with_options/stella-base-zh-v2/test.json" \

--output_json_dir="../data_clean/questions_with_knowledge/Mainland/test/retrive_use_question_with_options/stella-base-zh-v2/test_with_explain_using_moonshot-v1-32k_kimi" \

--list_placeholder="list_placeholder" \

--llm_output_key="chatgpt_explain" \

--index_key_name="id" \

--temperature=0.7 \

--max_tokens=4096 \

--top_p=0.95



#Verification set

python ./MultiProcessingLLM/multiprocess_using_chatgpt_input_with_prompt_and_json_data.py --do_check \

--num_process=30 \

--model_name="moonshot-v1-32k_kimi" \

--prompt_config_path="./MultiProcessingLLM/prompt_config_of_generate_explain.json" \

--use_load_raw_json_data_with_process \

--input_json_data_path="../data_clean/questions_with_knowledge/Mainland/dev/retrive_use_question_with_options/stella-base-zh-v2/dev.json" \

--output_json_dir="../data_clean/questions_with_knowledge/Mainland/dev/retrive_use_question_with_options/stella-base-zh-v2/dev_with_explain_using_moonshot-v1-32k_kimi" \

--list_placeholder="list_placeholder" \

--llm_output_key="chatgpt_explain" \

--index_key_name="id" \

--temperature=0.7 \

--max_tokens=4096 \

--top_p=0.95

Building a standard dataset for Chinese dataset (completed, no need for repeated operations)

python format_json_dataset_for_training_llm.py \

--input_json_data_path="../data_clean/questions_with_knowledge/Mainland/train/retrive_use_question_with_options/stella-base-zh-v2/train_with_explain_using_moonshot-v1-32k_kimi" \

--dataset_info_path=""../../dataset_info.json \

--dataset_name="RAG_MedQA_Mainland_train" \

--dataset_info_relative_dir="MedQA/" \

--output_json_data_dir="../" \

--do_register_dataset


#Build a test set

python format_json_dataset_for_training_llm.py \

--input_json_data_path="../data_clean/questions_with_knowledge/Mainland/test/retrive_use_question_with_options/stella-base-zh-v2/test_with_explain_using_moonshot-v1-32k_kimi" \

--dataset_info_path=""../../dataset_info.json \

--dataset_name="RAG_MedQA_Mainland_test" \

--dataset_info_relative_dir="MedQA/" \

--output_json_data_dir="../" \

--do_register_dataset

Fine tuning and testing

The instructions for fine-tuning and testing are mainly stored in/Below train_madel. 


Fine tuning

Before fine-tuning, modifications are needed/The following variables are used for train_madel/run_stngle_gpu_train_madel.sh


Meaning or purpose of variable names

The root directory of the ROOT BASE-DIR project is currently/home/chenyirong/MedQA_Train. Please modify it according to the actual situation. Its subdirectories include data, train_madel, etc

MODEL_NAME_OR-PATH is important. It is the path of the base model. Please modify it to the storage path in the server of the actual base model used!!!

The dataset used for DATASET training is/home/zhuxiang001/lamafactory/LaMA Factory/data//Defined in data/dataset_infojson, this is when calling/The data/MedQA/tiles/format_json is registered when the data_dataset_for_training.llm. py file is used

Modify the format to: "RAG_MedQA_Sainland_train_500 (example)": {"filename": "RAG_MedQA_Sainland_train_500 (example). json", "formatting": "sharegpt", "columns": {"messages": "messages"}, "tags": {"role_tag": "role", "content_tag": "content", "user_tag": "user", "assistant_tag": "assistant", "system_tag": "system"}


|Result_SASE_SAVE-DIR | Save path for training results, please set according to the actual situation | | NUM_TRAIN_EPOCHS | The number of epochs set for training, please set according to the actual situation | | llamafactory-cli train framework parameters | | | -- template | Need to be modified according to the base model, otherwise an error will occur, please refer to the projecthttps://github.com/hiyouga/LLaMA-FactorySet up the src/lmtuner/data/templating. py file in. | |--Cutoff_1en | The truncation length is set according to the actual situation, which is determined by the dataset and also linearly affects the amount of video memory used! | |--Per_device_train_match_2 | The batch_2 during training, which linearly affects the memory usage | | Other parameters | Refer to llamafactory-cli train-h or view in detailhttps://github.com/hiyouga/LLaMA-Factory|


conda activate llama_factory

cd ./train_model

#The first parameter specifies the value of lora_rank, which is an integer. The larger the value, the more video memory it occupies

#The second parameter specifies the learning rate during fine-tuning, usually taken as 0.0001, 0.0005, 0.00005, 0.00001, etc

chmod +x run_single_gpu_train_model.sh

CUDA_VISIBLE_DEVICES=0 ./run_single_gpu_train_model.sh 8 0.0001

test

Before testing, modifications are required/The following variables are used for train_madel/run_stngle_gpu_predict_madel.sh


Meaning or purpose of variable names

The root directory of the ROOT BASE-DIR project is currently/home/chenyirong/MedQA_Train. Please modify it according to the actual situation. Its subdirectories include data, train_madel, etc

MODEL_NAME_OR-PATH is important. It is the path of the base model. Please modify it to the storage path in the server of the actual base model used!!!

The path to save model parameters during the fine-tuning of ADAPTER-NAME_0R_PATH Lora. Specifically, its format includes checkpoint_steps, which can be set as an absolute path to ignore the passed checkpoint_steps

The dataset used for DATASET testing needs to be defined in/home/zhuxiang001/lamafactory/LaMA Factory/data/(./data/dataset_info. json), which is called/The data/MedQA/tiles/format_json is registered when the data_dataset_for_training.llm. py file is used

Modify the format to: "RAG_MedQA_mainland_test_300": {"filename": "RAG_MedQA_mainland_test_300. json", "formatting": "sharegpt", "columns": {"messages": "messages"}, "tags": {"role_tag": "role", "content_tag": "content", "user_tag": "user", "assistant_tag": "assistant", "system_tag": "system"}


|Result_SASE_SAVE-DIR | The path for saving inference results, please set the framework parameters of llamafactory-cli train according to the actual situation | | | -- template | needs to be modified according to the base model, otherwise an error will be reported. Please refer to the project for detailshttps://github.com/hiyouga/LLaMA-FactorySet up the src/lmtuner/data/templating. py file in|Other parameters | Refer to llamafactory-cli train-h or view in detailhttps://github.com/hiyouga/LLaMA-Factory|


conda activate llama_factory

cd ./train_model

chmod +x run_single_gpu_predict_model.sh

#The first parameter specifies the temperature during inference

#The second parameter specifies the top_p during inference

#The third parameter specifies the selected checkpoint, which should be chosen based on the actual situation

CUDA_VISIBLE_DEVICES=0 ./run_single_gpu_predict_model.sh 0.3 0.7 2000

Standardization of test results and statistical accuracy

Refer to the following command, where -- input_json _data_math specifies the corresponding JSON dataset path, which corresponds to the test set--Input_jsonl_data_math specifies the path for saving the results during inference--Output. xlsx_result_dath specifies the path to save the. xlsx output


conda activate llama_factory

cd ./train_model

python format_json_dataset_for_eval_from_jsonl.py \

--input_json_data_path="/home/phd-chen.yirong/scutcyr/LLaMA-Factory/data/MedQA/RAG_MedQA_Mainland_test_300.json" \

--input_jsonl_data_path="./results/predict_RAG_MedQA_Mainland_test_300_use_jiyichat_on_RAG_MedQA_Mainland_train-checkpoint-4500_with_temperature_0.7_top_p_0.8_top_k_20/generated_predictions.jsonl" \

--output_xlsx_result_path="./results/predict_RAG_MedQA_Mainland_test_300_use_jiyichat_on_RAG_MedQA_Mainland_train-checkpoint-4500_with_temperature_0.7_top_p_0.8_top_k_20.xlsx" 


Batch testing

Main call/Batch testing of the train_madel/run_maultiple_gpu_predict_madel.sh file requires modifications to ROOT BASE-DIR, MODEL NAME, DATASET, and/Corresponding to train_madel/run_stngle_gpu_predict_madel.sh; CUDA_VISIBLEDEVICE_SIST, temperatures, top-ps, and checkpoints need to be set according to the actual situation. 


conda activate llama_factory

cd ./train_model

chmod +x run_single_gpu_predict_model.sh

chmod +x run_multiple_gpu_predict_model.sh

./run_multiple_gpu_predict_model.sh

Batch statistics score: 


python format_json_dataset_for_eval_from_jsonl.py \

--input_json_data_path="/home/phd-chen.yirong/scutcyr/LLaMA-Factory/data/MedQA/RAG_MedQA_Mainland_test_300.json" \

--input_jsonl_data_path="./results/" \

--input_jsonl_subdir_startwiths="predict_RAG_MedQA_Mainland_test_300_use_jiyichat_on_RAG_MedQA_Mainland_train_checkpoint" \

--output_xlsx_result_path="./results/output_xlsx_result" \

--output_xlsx_total_result_path="./results/output_xlsx_result/0_all_scores.xlsx" 
